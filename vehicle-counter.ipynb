{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP5CjXDk4JxCZp70yEhlFam"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## 1. Project Setup: Create Directories\n","\n","First, we'll create the basic folder structure for our application. We need a `backend` folder to hold our Python server code and a `frontend` folder for the HTML, CSS, and JavaScript files."],"metadata":{"id":"blglr-eYpQyT"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"ULOcOTXYO-dm","executionInfo":{"status":"ok","timestamp":1758636542432,"user_tz":-360,"elapsed":19,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}}},"outputs":[],"source":["%%bash\n","mkdir -p /content/vehicle-counter/backend\n","mkdir -p /content/vehicle-counter/frontend"]},{"cell_type":"markdown","source":["### Backend Code: The SORT Tracker\n","\n","This cell writes the Python code for the **SORT (Simple Online and Realtime Tracking)** algorithm into a file named `sort_tracker.py`. This script is a core component of our backend; it's responsible for assigning a consistent ID to each detected vehicle across multiple frames, allowing us to track its movement."],"metadata":{"id":"cEALjvzqpV0-"}},{"cell_type":"code","source":["%%writefile /content/vehicle-counter/backend/sort_tracker.py\n","\"\"\"\n","This module provides an implementation of the SORT (Simple Online and Realtime Tracking)\n","algorithm, a pragmatic approach for multi-object tracking with a focus on simplicity and speed.\n","\n","The algorithm uses a Kalman filter for motion prediction and the Hungarian algorithm for data\n","association. It's designed to be effective for tracking objects like pedestrians or vehicles in\n","video streams.\n","\n","Core Components:\n","- KalmanBoxTracker: Manages the state of a single tracked object using a Kalman filter.\n","- Sort: The main class that orchestrates the tracking process across multiple objects and frames.\n","- Helper functions: For calculating Intersection over Union (IoU) and associating detections\n","  to trackers.\n","\"\"\"\n","from typing import List, Tuple\n","import numpy as np\n","from filterpy.kalman import KalmanFilter\n","from scipy.optimize import linear_sum_assignment\n","\n","\n","def iou(bb_test: np.ndarray, bb_gt: np.ndarray) -> float:\n","    \"\"\"\n","    Computes Intersection over Union (IoU) for a single pair of bounding boxes.\n","\n","    Args:\n","        bb_test (np.ndarray): Bounding box [x1, y1, x2, y2].\n","        bb_gt (np.ndarray): Ground truth bounding box [x1, y1, x2, y2].\n","\n","    Returns:\n","        float: The IoU value.\n","    \"\"\"\n","    xx1 = max(bb_test[0], bb_gt[0])\n","    yy1 = max(bb_test[1], bb_gt[1])\n","    xx2 = min(bb_test[2], bb_gt[2])\n","    yy2 = min(bb_test[3], bb_gt[3])\n","    w = max(0.0, xx2 - xx1)\n","    h = max(0.0, yy2 - yy1)\n","    inter = w * h\n","    area1 = max(0.0, (bb_test[2] - bb_test[0])) * max(0.0, (bb_test[3] - bb_test[1]))\n","    area2 = max(0.0, (bb_gt[2] - bb_gt[0])) * max(0.0, (bb_gt[3] - bb_gt[1]))\n","    union = area1 + area2 - inter\n","    return inter / union if union > 0 else 0.0\n","\n","\n","def iou_batch(dets: np.ndarray, trks: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Computes Intersection over Union (IoU) for two sets of bounding boxes in a vectorized manner.\n","\n","    Args:\n","        dets (np.ndarray): Detections, shape (N, 4) where N is the number of detections.\n","        trks (np.ndarray): Tracked objects, shape (M, 4) where M is the number of trackers.\n","\n","    Returns:\n","        np.ndarray: An (N, M) matrix with the IoU values.\n","    \"\"\"\n","    if dets.size == 0 or trks.size == 0:\n","        return np.zeros((dets.shape[0], trks.shape[0]), dtype=np.float32)\n","\n","    # Vectorized computation of IoU\n","    dets_exp = dets[:, None, :]\n","    trks_exp = trks[None, :, :]\n","\n","    xx1 = np.maximum(dets_exp[..., 0], trks_exp[..., 0])\n","    yy1 = np.maximum(dets_exp[..., 1], trks_exp[..., 1])\n","    xx2 = np.minimum(dets_exp[..., 2], trks_exp[..., 2])\n","    yy2 = np.minimum(dets_exp[..., 3], trks_exp[..., 3])\n","\n","    w = np.clip(xx2 - xx1, 0.0, None)\n","    h = np.clip(yy2 - yy1, 0.0, None)\n","    inter = w * h\n","\n","    dets_area = np.clip(dets_exp[..., 2] - dets_exp[..., 0], 0.0, None) * np.clip(\n","        dets_exp[..., 3] - dets_exp[..., 1], 0.0, None\n","    )\n","    trks_area = np.clip(trks_exp[..., 2] - trks_exp[..., 0], 0.0, None) * np.clip(\n","        trks_exp[..., 3] - trks_exp[..., 1], 0.0, None\n","    )\n","\n","    union = dets_area + trks_area - inter\n","    eps = np.finfo(np.float32).eps\n","    return np.where(union > 0.0, inter / (union + eps), 0.0).astype(np.float32)\n","\n","\n","def convert_bbox_to_z(bbox: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Takes a bounding box in the form [x1, y1, x2, y2] and returns z in the form\n","    [x, y, s, r] where x,y is the centre of the box, s is the scale/area, and r is\n","    the aspect ratio.\n","\n","    Args:\n","        bbox (np.ndarray): Bounding box in [x1, y1, x2, y2] format.\n","\n","    Returns:\n","        np.ndarray: Kalman filter measurement vector [x, y, s, r].\n","    \"\"\"\n","    w = bbox[2] - bbox[0]\n","    h = bbox[3] - bbox[1]\n","    x = bbox[0] + w / 2.0\n","    y = bbox[1] + h / 2.0\n","    s = w * h  # scale is just area\n","    r = w / (h + 1e-6)\n","    return np.array([x, y, s, r]).reshape((4, 1))\n","\n","\n","def convert_x_to_bbox(x: np.ndarray) -> np.ndarray:\n","    \"\"\"\n","    Takes a bounding box in the centre form [x, y, s, r] and returns it in the form\n","    [x1, y1, x2, y2] where x1, y1 is the top-left and x2, y2 is the bottom-right.\n","\n","    Args:\n","        x (np.ndarray): Kalman filter state vector [x, y, s, r, ...].\n","\n","    Returns:\n","        np.ndarray: Bounding box in [x1, y1, x2, y2] format.\n","    \"\"\"\n","    x_c, y_c, s, r = x[0], x[1], max(1.0, x[2]), max(1e-6, x[3])\n","    w = np.sqrt(s * r)\n","    h = s / (w + 1e-6)\n","    x1 = x_c - w / 2.0\n","    y1 = y_c - h / 2.0\n","    x2 = x_c + w / 2.0\n","    y2 = y_c + h / 2.0\n","    return np.array([x1, y1, x2, y2]).reshape((1, 4))\n","\n","\n","class KalmanBoxTracker:\n","    \"\"\"\n","    This class represents the internal state of individual tracked objects observed as bbox.\n","    It uses a Kalman filter to predict the object's position in subsequent frames.\n","    \"\"\"\n","\n","    _count = 0\n","\n","    def __init__(self, bbox: np.ndarray):\n","        \"\"\"\n","        Initializes a tracker using initial bounding box.\n","\n","        The state is [x, y, s, r, dx, dy, ds, dr], where (x, y) is the center,\n","        s is the scale/area, r is the aspect ratio, and the rest are velocities.\n","        However, the aspect ratio velocity 'dr' is not used in this model.\n","        \"\"\"\n","        # Define constant velocity model\n","        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n","        dt = 1.0\n","        # State Transition Matrix\n","        self.kf.F = np.array(\n","            [\n","                [1, 0, 0, 0, dt, 0, 0],\n","                [0, 1, 0, 0, 0, dt, 0],\n","                [0, 0, 1, 0, 0, 0, dt],\n","                [0, 0, 0, 1, 0, 0, 0],\n","                [0, 0, 0, 0, 1, 0, 0],\n","                [0, 0, 0, 0, 0, 1, 0],\n","                [0, 0, 0, 0, 0, 0, 1],\n","            ]\n","        )\n","        # Measurement Matrix\n","        self.kf.H = np.array(\n","            [\n","                [1, 0, 0, 0, 0, 0, 0],\n","                [0, 1, 0, 0, 0, 0, 0],\n","                [0, 0, 1, 0, 0, 0, 0],\n","                [0, 0, 0, 1, 0, 0, 0],\n","            ]\n","        )\n","        # Measurement Noise Covariance\n","        self.kf.R[2:, 2:] *= 10.0\n","        # Process Covariance\n","        self.kf.P[4:, 4:] *= 1000.0  # give high uncertainty to the unobservable initial velocities\n","        self.kf.P *= 10.0\n","        # Process Noise\n","        self.kf.Q[-1, -1] *= 0.01\n","        self.kf.Q[4:, 4:] *= 0.01\n","\n","        self.kf.x[:4] = convert_bbox_to_z(bbox)\n","\n","        self.time_since_update = 0\n","        self.id = KalmanBoxTracker._count\n","        KalmanBoxTracker._count += 1\n","        self.hits = 1\n","        self.hit_streak = 1\n","        self.age = 0\n","\n","    def update(self, bbox: np.ndarray):\n","        \"\"\"\n","        Updates the state vector with observed bbox.\n","        \"\"\"\n","        self.time_since_update = 0\n","        self.hits += 1\n","        self.hit_streak += 1\n","        self.kf.update(convert_bbox_to_z(bbox))\n","\n","    def predict(self) -> np.ndarray:\n","        \"\"\"\n","        Advances the state vector and returns the predicted bounding box estimate.\n","        \"\"\"\n","        self.kf.predict()\n","        self.age += 1\n","        if self.time_since_update > 0:\n","            self.hit_streak = 0\n","        self.time_since_update += 1\n","        return convert_x_to_bbox(self.kf.x)\n","\n","    def get_state(self) -> np.ndarray:\n","        \"\"\"\n","        Returns the current bounding box estimate.\n","        \"\"\"\n","        return convert_x_to_bbox(self.kf.x)\n","\n","\n","def associate_detections_to_trackers(\n","    detections: np.ndarray, trackers: np.ndarray, iou_threshold: float\n",") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n","    \"\"\"\n","    Assigns detections to tracked object (both represented as bounding boxes).\n","\n","    Args:\n","        detections (np.ndarray): A set of detected bounding boxes.\n","        trackers (np.ndarray): A set of bounding boxes predicted by trackers.\n","        iou_threshold (float): The IoU threshold for considering a match.\n","\n","    Returns:\n","        A tuple of three arrays:\n","        - matches (np.ndarray): Array of shape (N, 2) with indices of matched (detection, tracker).\n","        - unmatched_dets (np.ndarray): Array of indices of unmatched detections.\n","        - unmatched_trks (np.ndarray): Array of indices of unmatched trackers.\n","    \"\"\"\n","    if trackers.size == 0 or detections.size == 0:\n","        return (\n","            np.empty((0, 2), dtype=int),\n","            np.arange(detections.shape[0]),\n","            np.arange(trackers.shape[0]),\n","        )\n","\n","    ious = iou_batch(detections, trackers)\n","    cost = 1.0 - ious\n","\n","    # Use the Hungarian algorithm (linear_sum_assignment) to find optimal assignments\n","    det_idx, trk_idx = linear_sum_assignment(cost)\n","\n","    matches = []\n","    unmatched_dets = set(range(detections.shape[0]))\n","    unmatched_trks = set(range(trackers.shape[0]))\n","\n","    for d, t in zip(det_idx, trk_idx):\n","        if ious[d, t] >= iou_threshold:\n","            matches.append([d, t])\n","            unmatched_dets.discard(d)\n","            unmatched_trks.discard(t)\n","\n","    return (\n","        np.array(matches, dtype=int) if matches else np.empty((0, 2), dtype=int),\n","        np.array(sorted(list(unmatched_dets)), dtype=int),\n","        np.array(sorted(list(unmatched_trks)), dtype=int),\n","    )\n","\n","\n","class Sort:\n","    \"\"\"\n","    Sort is the main tracking class. It takes object detections for each frame\n","    and manages the lifecycle of trackers.\n","    \"\"\"\n","\n","    def __init__(self, max_age: int = 20, min_hits: int = 3, iou_threshold: float = 0.3):\n","        \"\"\"\n","        Initializes the Sort tracker.\n","\n","        Args:\n","            max_age (int): Maximum number of frames to keep a track alive without new detections.\n","            min_hits (int): Minimum number of consecutive detections to start a track.\n","            iou_threshold (float): IoU threshold for matching detections to existing tracks.\n","        \"\"\"\n","        self.max_age = max_age\n","        self.min_hits = min_hits\n","        self.iou_threshold = iou_threshold\n","        self.trackers: List[KalmanBoxTracker] = []\n","        self.frame_count = 0\n","        KalmanBoxTracker._count = 0  # Reset tracker ID count\n","\n","    def update(self, dets: np.ndarray) -> np.ndarray:\n","        \"\"\"\n","        This method is the main update loop. It should be called for each frame.\n","\n","        Args:\n","            dets (np.ndarray): A numpy array of detections in the format [[x1,y1,x2,y2,score],...].\n","                               If no detections are present, it should be an empty array.\n","\n","        Returns:\n","            np.ndarray: A numpy array of tracked objects in the format [[x1,y1,x2,y2,track_id],...].\n","        \"\"\"\n","        self.frame_count += 1\n","\n","        # 1. Predict new locations of existing trackers\n","        trks = []\n","        for t in self.trackers:\n","            pos = t.predict()\n","            trks.append(pos.reshape(-1))\n","        trks = np.array(trks) if len(trks) > 0 else np.empty((0, 4))\n","\n","        # 2. Associate detections with predicted tracker locations\n","        detection_bboxes = dets[:, :4] if dets.size else np.empty((0, 4))\n","        matches, unmatched_dets, unmatched_trks = associate_detections_to_trackers(\n","            detection_bboxes, trks, self.iou_threshold\n","        )\n","\n","        # 3. Update matched trackers with new detection info\n","        for m in matches:\n","            trk_idx = m[1]\n","            det_idx = m[0]\n","            self.trackers[trk_idx].update(dets[det_idx, :4])\n","\n","        # 4. Create new trackers for unmatched detections\n","        for i in unmatched_dets:\n","            self.trackers.append(KalmanBoxTracker(dets[i, :4]))\n","\n","        # 5. Manage tracker lifecycle and prepare output\n","        ret = []\n","        alive_trackers = []\n","        for t in self.trackers:\n","            # A track is considered confirmed if it has been updated recently and has a sufficient hit streak.\n","            # We also include tracks early on (before min_hits frames have passed) to get initial results.\n","            if (t.time_since_update < 1) and (\n","                t.hit_streak >= self.min_hits or self.frame_count <= self.min_hits\n","            ):\n","                d = t.get_state().reshape(-1)\n","                ret.append(np.concatenate([d, [t.id]], axis=0))\n","\n","            # Remove trackers that have been lost for too long\n","            if t.time_since_update <= self.max_age:\n","                alive_trackers.append(t)\n","\n","        self.trackers = alive_trackers\n","\n","        return np.array(ret) if len(ret) > 0 else np.empty((0, 5))\n","\n","\n","if __name__ == \"__main__\":\n","    # --- Example Usage ---\n","\n","    # Create an instance of the Sort tracker\n","    tracker = Sort(max_age=5, min_hits=2, iou_threshold=0.3)\n","\n","    # Simulate frames from a video\n","    # Each frame contains a list of detections: [x1, y1, x2, y2, confidence_score]\n","    simulated_frames = [\n","        # Frame 1: One object detected\n","        np.array([[100, 100, 150, 150, 0.9]]),\n","        # Frame 2: The object moved slightly\n","        np.array([[105, 105, 155, 155, 0.92]]),\n","        # Frame 3: Object continues to move, and a new object appears\n","        np.array([[110, 110, 160, 160, 0.93], [300, 200, 350, 250, 0.88]]),\n","        # Frame 4: Both objects move\n","        np.array([[115, 115, 165, 165, 0.94], [305, 205, 355, 255, 0.89]]),\n","        # Frame 5: First object is missed by the detector\n","        np.array([[310, 210, 360, 260, 0.90]]),\n","        # Frame 6: First object is detected again, second one continues\n","        np.array([[125, 125, 175, 175, 0.91], [315, 215, 365, 265, 0.91]]),\n","    ]\n","\n","    print(\"--- Running SORT Tracker Simulation ---\")\n","    for frame_num, detections in enumerate(simulated_frames):\n","        print(f\"\\n--- Frame {frame_num + 1} ---\")\n","        print(f\"Detections:\\n{detections}\")\n","\n","        # Update the tracker with the new detections\n","        tracked_objects = tracker.update(detections)\n","\n","        print(f\"Tracked Objects (Output format: [x1, y1, x2, y2, track_id]):\\n{tracked_objects}\")"],"metadata":{"id":"iTOQZsI-QHfE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636542521,"user_tz":-360,"elapsed":72,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"43c0f43e-3519-4649-f1bb-480b18100582"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/vehicle-counter/backend/sort_tracker.py\n"]}]},{"cell_type":"markdown","source":["### Backend Code: The Main Flask Application\n","\n","This is the heart of our backend. This cell writes the main application logic to `app.py`. This Python script uses the **Flask** framework to create a web server that does the following:\n","* Loads the **YOLOv8 model** to detect vehicles.\n","* Uses the **SORT tracker** to track the detected vehicles.\n","* Processes a video file frame-by-frame, draws bounding boxes, and counts vehicles crossing a virtual line.\n","* Provides several **API endpoints**:\n","    * `/video_feed`: Streams the processed video.\n","    * `/api/counts`: Returns the latest vehicle counts.\n","    * `/api/metrics`: Provides performance data like FPS.\n","    * `/api/reset`: Resets the counters."],"metadata":{"id":"E5lrmdY5pokW"}},{"cell_type":"code","source":["%%writefile /content/vehicle-counter/backend/app.py\n","\"\"\"\n","Real-time Vehicle Counting Web Application using YOLOv8 and Flask.\n","\n","This script launches a Flask web server that performs real-time vehicle detection\n","and tracking on a video stream. It serves an annotated video feed and provides\n","API endpoints to retrieve vehicle counts and performance metrics.\n","\n","Core Functionality:\n","- Ingests a video file specified by the VIDEO_PATH environment variable.\n","- Uses a YOLOv8 model for object detection.\n","- Employs either the SORT or ByteTrack algorithm for object tracking.\n","- Draws bounding boxes, track IDs, and trails on each frame.\n","- Counts vehicles as they cross a predefined horizontal line.\n","- Streams the processed video to a web browser via an MJPEG stream.\n","- Provides a REST API for accessing counts, metrics, and resetting the state.\n","\n","Configuration is managed via environment variables. Key variables include:\n","- VIDEO_PATH: Path to the input video file.\n","- MODEL_PATH: Path to the YOLOv8 model file (e.g., yolov8s.pt).\n","- TRACKER_MODE: The tracking algorithm to use, either \"SORT\" or \"BYTE\".\n","- CONF_THRESH: Confidence threshold for object detection.\n","- LINE_Y: The vertical position of the counting line.\n","\"\"\"\n","import os\n","import time\n","import threading\n","from collections import deque, defaultdict\n","\n","import cv2\n","import numpy as np\n","import torch\n","from flask import Flask, Response, jsonify\n","from flask_cors import CORS\n","from ultralytics import YOLO\n","\n","# Assuming sort_tracker.py containing the Sort class is in the same directory.\n","from sort_tracker import Sort\n","\n","# --- Configuration from Environment Variables ---\n","VIDEO_PATH = os.environ.get(\"VIDEO_PATH\", \"../content/video1.mp4\")\n","MODEL_PATH = os.environ.get(\"MODEL_PATH\", \"yolov8s.pt\")\n","MODEL_DEVICE = os.environ.get('MODEL_DEVICE', 'auto').lower()\n","CONF_THRESH = float(os.environ.get(\"CONF_THRESH\", 0.35))\n","IOU_THRESH = float(os.environ.get(\"IOU_THRESH\", 0.5))\n","DEFAULT_LINE_Y = int(os.environ.get(\"LINE_Y\", 360))\n","TRAIL_LEN = int(os.environ.get(\"TRAIL_LEN\", 20))\n","RESIZE_WIDTH = int(os.environ.get(\"RESIZE_WIDTH\", 960))\n","JPEG_QUALITY = int(os.environ.get(\"JPEG_QUALITY\", 80))\n","RESET_ON_LOOP = os.environ.get(\"RESET_ON_LOOP\", \"false\").lower() in {\"1\", \"true\", \"yes\"}\n","TRACKER_MODE = os.environ.get(\"TRACKER_MODE\", \"SORT\").upper()\n","\n","# --- Constants ---\n","VEHICLE_NAMES = {\"car\", \"truck\", \"bus\", \"motorcycle\"}\n","TEXT_COLOR = (255, 255, 255)  # White\n","LINE_COLOR = (0, 255, 255)   # Cyan\n","BOX_COLOR = (0, 255, 0)      # Green\n","ID_COLOR = (255, 0, 0)       # Blue\n","\n","# --- Application Setup ---\n","app = Flask(__name__)\n","# Enable Cross-Origin Resource Sharing for API and video feed routes.\n","CORS(app, resources={r\"/api/*\": {\"origins\": \"*\"}, r\"/video_feed\": {\"origins\": \"*\"}, r\"/health\": {\"origins\": \"*\"}})\n","\n","# --- Model and Tracker Initialization ---\n","# Load the YOLOv8 model from the specified path.\n","model = YOLO(MODEL_PATH)\n","# Determine the active device for the model (CUDA or CPU).\n","if MODEL_DEVICE == 'auto':\n","    MODEL_DEVICE_ACTIVE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","else:\n","    MODEL_DEVICE_ACTIVE = MODEL_DEVICE\n","# Move the model to the selected device, with a fallback to CPU.\n","try:\n","    model.to(MODEL_DEVICE_ACTIVE)\n","except Exception as exc:\n","    print(f'[warn] Failed to move model to {MODEL_DEVICE_ACTIVE}: {exc}')\n","    MODEL_DEVICE_ACTIVE = 'cpu'\n","    try:\n","        model.to(MODEL_DEVICE_ACTIVE)\n","    except Exception as exc_cpu:\n","        print(f'[warn] Failed to move model to cpu: {exc_cpu}')\n","\n","# Open the video file for processing.\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","if not cap.isOpened():\n","    raise RuntimeError(f\"Could not open video: {VIDEO_PATH}\")\n","\n","# Initialize the SORT tracker if selected.\n","mot = Sort(max_age=20, min_hits=3, iou_threshold=0.3)\n","\n","# --- Global State Variables ---\n","# Dictionary to store vehicle counts, protected by a lock for thread safety.\n","counts = {\"up\": 0, \"down\": 0}\n","counts_lock = threading.Lock()\n","\n","# Dictionary to store the history of each tracked object.\n","track_history = defaultdict(lambda: {\n","    \"centers\": deque(maxlen=TRAIL_LEN), # Stores recent center points for drawing trails.\n","    \"last_side\": None,                  # 'above' or 'below' the line.\n","    \"last_seen\": time.time(),           # Timestamp of the last update.\n","})\n","\n","# Dictionary for performance metrics, also protected by a lock.\n","metrics = {\n","    \"start_time\": time.time(),\n","    \"frames\": 0,\n","    \"recent_ts\": deque(maxlen=120),    # Timestamps of recent frames for FPS calculation.\n","    \"fps\": 0.0,\n","    \"inference_ms\": deque(maxlen=240), # Inference times for averaging.\n","    \"avg_inference_ms\": 0.0,\n","    \"model_device\": MODEL_DEVICE_ACTIVE,\n","}\n","metrics_lock = threading.Lock()\n","\n","\n","def preprocess(frame):\n","    \"\"\"\n","    Resizes the frame to a standard width if required.\n","\n","    Args:\n","        frame (np.ndarray): The input video frame.\n","\n","    Returns:\n","        np.ndarray: The resized frame.\n","    \"\"\"\n","    if RESIZE_WIDTH > 0:\n","        h, w = frame.shape[:2]\n","        if w != RESIZE_WIDTH:\n","            scale = RESIZE_WIDTH / float(w)\n","            frame = cv2.resize(frame, (RESIZE_WIDTH, int(h * scale)))\n","    return frame\n","\n","\n","def update_counts_for_crossing(tid, cy, line_y):\n","    \"\"\"\n","    Updates the up/down counts when a tracked object crosses the line.\n","\n","    Args:\n","        tid (int): The track ID of the object.\n","        cy (int): The current y-coordinate of the object's center.\n","        line_y (int): The y-coordinate of the counting line.\n","    \"\"\"\n","    info = track_history[tid]\n","    last_side = info[\"last_side\"]\n","    # Determine the current side of the line.\n","    side = \"below\" if cy > line_y else \"above\"\n","    # If this is the first time we see this object, just record its side.\n","    if last_side is None:\n","        info[\"last_side\"] = side\n","        return\n","    # If the object has crossed the line, update the count.\n","    if side != last_side:\n","        direction = \"down\" if side == \"below\" and last_side == \"above\" else \"up\"\n","        with counts_lock:\n","            counts[direction] += 1\n","        # Update the side for the next frame.\n","        info[\"last_side\"] = side\n","\n","\n","def draw_overlays(frame, line_y, tracks):\n","    \"\"\"\n","    Draws all visual annotations onto the frame.\n","\n","    This includes the counting line, bounding boxes for tracked objects,\n","    track IDs, object trails, and the current vehicle counts.\n","\n","    Args:\n","        frame (np.ndarray): The video frame to draw on.\n","        line_y (int): The y-coordinate of the counting line.\n","        tracks (np.ndarray): An array of active tracks from the tracker.\n","\n","    Returns:\n","        np.ndarray: The frame with overlays drawn on it.\n","    \"\"\"\n","    h, w = frame.shape[:2]\n","    # Draw the counting line.\n","    cv2.line(frame, (0, line_y), (w, line_y), LINE_COLOR, 2)\n","    cv2.putText(frame, f\"Line y={line_y}\", (10, max(25, line_y - 8)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, LINE_COLOR, 2, cv2.LINE_AA)\n","    # Draw boxes, IDs, and trails for each active track.\n","    for x1, y1, x2, y2, tid in tracks.astype(int):\n","        cv2.rectangle(frame, (x1, y1), (x2, y2), BOX_COLOR, 2)\n","        cv2.putText(frame, f\"ID {tid}\", (x1, max(0, y1 - 6)), cv2.FONT_HERSHEY_SIMPLEX, 0.6, ID_COLOR, 2, cv2.LINE_AA)\n","        centers = track_history[tid][\"centers\"]\n","        # Draw the trail.\n","        for i in range(1, len(centers)):\n","            if centers[i - 1] and centers[i]:\n","                cv2.line(frame, centers[i - 1], centers[i], (200, 200, 200), 2)\n","    # Draw the total counts on the screen.\n","    with counts_lock:\n","        up, down = counts[\"up\"], counts[\"down\"]\n","    cv2.putText(frame, f\"Up: {up}   Down: {down}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1.0, TEXT_COLOR, 2, cv2.LINE_AA)\n","    return frame\n","\n","\n","def bump_metrics(inference_ms=None):\n","    \"\"\"\n","    Updates performance metrics such as FPS and average inference time.\n","\n","    Args:\n","        inference_ms (float, optional): The inference time for the current frame in milliseconds.\n","    \"\"\"\n","    now = time.time()\n","    with metrics_lock:\n","        metrics[\"frames\"] += 1\n","        metrics[\"recent_ts\"].append(now)\n","        # Calculate FPS over a sliding window of recent timestamps.\n","        if len(metrics[\"recent_ts\"]) >= 2:\n","            dt = metrics[\"recent_ts\"][-1] - metrics[\"recent_ts\"][0]\n","            if dt > 0:\n","                metrics[\"fps\"] = (len(metrics[\"recent_ts\"]) - 1) / dt\n","        # Update average inference time.\n","        if inference_ms is not None:\n","            metrics[\"inference_ms\"].append(inference_ms)\n","            metrics[\"avg_inference_ms\"] = sum(metrics[\"inference_ms\"]) / len(metrics[\"inference_ms\"])\n","\n","\n","def reset_state(full_reset_tracker=True):\n","    \"\"\"\n","    Resets the application's state, including counts and track history.\n","\n","    Args:\n","        full_reset_tracker (bool): If True and using SORT, re-initializes the tracker instance.\n","    \"\"\"\n","    global mot, track_history\n","    # Reset counts to zero.\n","    with counts_lock:\n","        counts[\"up\"] = 0\n","        counts[\"down\"] = 0\n","    # Clear the tracking history.\n","    track_history = defaultdict(lambda: {\n","        \"centers\": deque(maxlen=TRAIL_LEN),\n","        \"last_side\": None,\n","        \"last_seen\": time.time(),\n","    })\n","    # Optionally, create a fresh SORT tracker instance.\n","    if full_reset_tracker and TRACKER_MODE == \"SORT\":\n","        mot = Sort(max_age=20, min_hits=3, iou_threshold=0.3)\n","\n","\n","def frame_generator():\n","    \"\"\"\n","    A generator function that processes video frames and yields them for streaming.\n","\n","    This is the core processing loop. It reads frames from the video source,\n","    performs detection and tracking, updates counts, draws overlays, and\n","    encodes the frame as a JPEG for the MJPEG stream.\n","    \"\"\"\n","    global cap\n","    while True:\n","        ret, frame = cap.read()\n","        # If the video ends, loop back to the beginning.\n","        if not ret:\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n","            if RESET_ON_LOOP:\n","                reset_state(full_reset_tracker=True)\n","            continue\n","\n","        frame = preprocess(frame)\n","        line_y = DEFAULT_LINE_Y\n","        tracks_array = np.empty((0, 5), dtype=np.float32)\n","        inference_start = time.perf_counter()\n","\n","        # --- Detection and Tracking Logic ---\n","        if TRACKER_MODE == \"SORT\":\n","            # 1. Run detection.\n","            results = model(frame, conf=CONF_THRESH, iou=IOU_THRESH, verbose=False)[0]\n","            dets = []\n","            if results.boxes is not None and len(results.boxes) > 0:\n","                # 2. Filter detections for vehicle classes.\n","                for box in results.boxes:\n","                    name = results.names[int(box.cls)]\n","                    if name in VEHICLE_NAMES:\n","                        xyxy = box.xyxy.cpu().numpy().flatten()\n","                        p = float(box.conf.cpu())\n","                        dets.append([xyxy[0], xyxy[1], xyxy[2], xyxy[3], p])\n","            dets_np = np.array(dets, dtype=np.float32) if dets else np.empty((0, 5), dtype=np.float32)\n","            # 3. Update SORT tracker with the filtered detections.\n","            tracks_array = mot.update(dets_np)\n","        else: # ByteTrack\n","            # Use YOLO's built-in tracking which uses ByteTrack.\n","            results = model.track(frame, conf=CONF_THRESH, iou=IOU_THRESH, persist=True, verbose=False)[0]\n","            byte_tracks = []\n","            if results.boxes is not None and len(results.boxes) > 0 and results.boxes.id is not None:\n","                 # Filter tracks for vehicle classes.\n","                for box in results.boxes:\n","                    name = results.names[int(box.cls)]\n","                    if name in VEHICLE_NAMES:\n","                        xyxy = box.xyxy.cpu().numpy().flatten()\n","                        tid = int(box.id.cpu())\n","                        byte_tracks.append([xyxy[0], xyxy[1], xyxy[2], xyxy[3], float(tid)])\n","            tracks_array = np.asarray(byte_tracks, dtype=np.float32) if byte_tracks else np.empty((0, 5), dtype=np.float32)\n","\n","        inference_ms = (time.perf_counter() - inference_start) * 1000.0\n","        now = time.time()\n","\n","        # --- Update Track History and Counts ---\n","        active_ids = set()\n","        for x1, y1, x2, y2, tid in tracks_array:\n","            tid = int(tid)\n","            active_ids.add(tid)\n","            # Calculate the center of the bounding box.\n","            cx = int((x1 + x2) / 2.0)\n","            cy = int((y1 + y2) / 2.0)\n","            # Update the history for this track.\n","            track_history[tid][\"centers\"].append((cx, cy))\n","            track_history[tid][\"last_seen\"] = now\n","            # Check if the track crossed the line.\n","            update_counts_for_crossing(tid, cy, line_y)\n","\n","        # --- Clean Up Stale Tracks ---\n","        # Remove tracks that haven't been seen for a few seconds.\n","        stale_cutoff = now - 5.0\n","        for tid, meta in list(track_history.items()):\n","            if meta[\"last_seen\"] < stale_cutoff and tid not in active_ids:\n","                del track_history[tid]\n","\n","        # --- Encode and Yield Frame ---\n","        frame = draw_overlays(frame, line_y, tracks_array)\n","        # Encode the frame as JPEG.\n","        ok, jpeg = cv2.imencode(\".jpg\", frame, [int(cv2.IMWRITE_JPEG_QUALITY), JPEG_QUALITY])\n","        if not ok:\n","            continue\n","        # Update performance metrics.\n","        bump_metrics(inference_ms)\n","        # Yield the frame in the format required for MJPEG streaming.\n","        yield (b\"--frame\\r\\n\"\n","               b\"Content-Type: image/jpeg\\r\\n\\r\\n\" + jpeg.tobytes() + b\"\\r\\n\")\n","\n","# --- Flask API Routes ---\n","@app.route(\"/video_feed\")\n","def video_feed():\n","    \"\"\"Flask route to serve the MJPEG video stream.\"\"\"\n","    return Response(frame_generator(), mimetype=\"multipart/x-mixed-replace; boundary=frame\")\n","\n","\n","@app.route(\"/api/counts\")\n","def api_counts():\n","    \"\"\"Flask route to get the current vehicle counts as JSON.\"\"\"\n","    with counts_lock:\n","        return jsonify({\"up\": int(counts[\"up\"]), \"down\": int(counts[\"down\"])})\n","\n","\n","@app.route(\"/api/reset\")\n","def api_reset():\n","    \"\"\"Flask route to reset the application's state.\"\"\"\n","    reset_state(full_reset_tracker=True)\n","    return jsonify({\"status\": \"ok\", \"message\": \"Counts and state reset.\"})\n","\n","\n","@app.route(\"/api/metrics\")\n","def api_metrics():\n","    \"\"\"Flask route to get performance and configuration metrics as JSON.\"\"\"\n","    with metrics_lock:\n","        data = {\n","            \"fps\": round(metrics[\"fps\"], 2),\n","            \"avg_inference_ms\": round(metrics[\"avg_inference_ms\"], 2),\n","            \"model_device\": metrics[\"model_device\"],\n","            \"frames_processed\": int(metrics[\"frames\"]),\n","            \"uptime_sec\": int(time.time() - metrics[\"start_time\"]),\n","            \"tracker_mode\": TRACKER_MODE,\n","            \"reset_on_loop\": RESET_ON_LOOP,\n","            \"resize_width\": RESIZE_WIDTH,\n","            \"jpeg_quality\": JPEG_QUALITY,\n","            \"conf_thresh\": CONF_THRESH,\n","            \"iou_thresh\": IOU_THRESH,\n","            \"line_y\": DEFAULT_LINE_Y,\n","        }\n","    return jsonify(data)\n","\n","\n","@app.route(\"/health\")\n","def health():\n","    \"\"\"Flask route for a simple health check.\"\"\"\n","    return jsonify({\"ok\": True, \"service\": \"vehicle-counter\", \"tracker_mode\": TRACKER_MODE})\n","\n","\n","@app.route(\"/\")\n","def root():\n","    \"\"\"Flask route for the root URL.\"\"\"\n","    return jsonify({\"ok\": True, \"message\": \"Use /video_feed and /api/counts\"})\n","\n","\n","# --- Main Execution Block ---\n","if __name__ == \"__main__\":\n","    host = os.environ.get(\"HOST\", \"0.0.0.0\")\n","    port = int(os.environ.get(\"PORT\", \"5000\"))\n","    # Set some default environment variables if they are not already defined.\n","    # This can be useful for running the script directly without a .env file.\n","    os.environ.setdefault(\"RESIZE_WIDTH\", \"960\")\n","    os.environ.setdefault(\"JPEG_QUALITY\", \"80\")\n","    os.environ.setdefault(\"CONF_THRESH\", \"0.35\")\n","    os.environ.setdefault(\"IOU_THRESH\", \"0.50\")\n","    # Start the Flask development server.\n","    app.run(host=host, port=port, debug=False, threaded=True)"],"metadata":{"id":"3UTbq0slQOtm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636542619,"user_tz":-360,"elapsed":95,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"962d97c0-b65f-4280-ce8c-848d6701de5b"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/vehicle-counter/backend/app.py\n"]}]},{"cell_type":"markdown","metadata":{"id":"af24c35a"},"source":["### Mount Google Drive (Recommended)\n","\n","To use your own video file without re-uploading it every time, you can mount your Google Drive. This cell will prompt you for authorization. Once mounted, your Drive files will be accessible from this notebook at the path `/content/drive/My Drive/`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1e0c364a","executionInfo":{"status":"ok","timestamp":1758636568579,"user_tz":-360,"elapsed":25957,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"39c92afb-10fa-4d4d-9bb4-18d9cc2f3a3f"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"365fc8e9"},"source":["After running the cell above and following the authorization steps, your Google Drive will be accessible at `/content/drive/My Drive/`. You can then place your video file in your Drive and update the `VIDEO_PATH` environment variable in the setup cell to point to its location in your Drive (e.g., `/content/drive/My Drive/your_video_folder/video1.mp4`)."]},{"cell_type":"markdown","source":["## 2. Frontend Setup: HTML, CSS, and JavaScript\n","\n","Now, let's create the files for the user interface.\n","\n","### Frontend: HTML Structure\n","\n","This cell writes the `index.html` file. It defines the structure of the web page, including the header, the video display area, the live metrics cards, and the control buttons."],"metadata":{"id":"nHqOVNHCqByl"}},{"cell_type":"code","source":["%%writefile /content/vehicle-counter/frontend/index.html\n","<!doctype html>\n","<html lang=\"en\">\n","<head>\n","  <meta charset=\"utf-8\" />\n","  <title>Real‑Time Vehicle Counter (PoC)</title>\n","  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n","  <link href=\"https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap\" rel=\"stylesheet\" />\n","  <link rel=\"stylesheet\" href=\"./styles.css\" />\n","</head>\n","<body>\n","  <header class=\"app-header\">\n","    <h1>Real‑Time Vehicle Counter</h1>\n","    <p class=\"subtitle\">YOLOv8m + SORT / ByteTrack • Two‑way line crossing • Live counts</p>\n","  </header>\n","\n","  <main class=\"container\">\n","    <section class=\"video-card\">\n","      <div class=\"video-header\">\n","        <h2>Processed Stream</h2>\n","        <div class=\"status-bar\" id=\"statusBar\">\n","          <span class=\"dot\" id=\"statusDot\"></span>\n","          <span id=\"statusText\">Connecting…</span>\n","        </div>\n","      </div>\n","      <div class=\"video-wrap\">\n","        <div class=\"overlay\" id=\"loadingOverlay\">\n","          <div class=\"spinner\"></div>\n","          <div>Connecting to video stream…</div>\n","        </div>\n","        <img id=\"video\" alt=\"Live processed video stream\" />\n","      </div>\n","      <div class=\"controls\">\n","        <button id=\"btnReset\" class=\"btn primary\">Reset Counts</button>\n","        <button id=\"btnPause\" class=\"btn\">Pause</button>\n","        <button id=\"btnDownload\" class=\"btn\">Download CSV</button>\n","      </div>\n","    </section>\n","\n","    <section class=\"counts-card\">\n","      <h2>Live Metrics</h2>\n","      <div class=\"grid\">\n","        <div class=\"metric up\">\n","          <span class=\"label\">Up</span>\n","          <span id=\"count-up\" class=\"value\">0</span>\n","          <span id=\"rate-up\" class=\"hint\">0.0 / min</span>\n","        </div>\n","        <div class=\"metric down\">\n","          <span class=\"label\">Down</span>\n","          <span id=\"count-down\" class=\"value\">0</span>\n","          <span id=\"rate-down\" class=\"hint\">0.0 / min</span>\n","        </div>\n","        <div class=\"metric total\">\n","          <span class=\"label\">Total</span>\n","          <span id=\"count-total\" class=\"value\">0</span>\n","          <span class=\"hint\">Since last reset</span>\n","        </div>\n","        <div class=\"metric fps\">\n","          <span class=\"label\">Server FPS</span>\n","          <span id=\"fps\" class=\"value\">0</span>\n","          <span id=\"frames\" class=\"hint\">0 frames</span>\n","        </div>\n","      </div>\n","      <div class=\"meta\">\n","        <span id=\"last-updated\">Last updated: —</span>\n","      </div>\n","    </section>\n","  </main>\n","\n","  <footer class=\"app-footer\">\n","    <small>\n","      Decoupled front end • Set backend via <code>?backend=&lt;URL&gt;</code>.\n","      Current: <span id=\"backendUrl\"></span>\n","    </small>\n","  </footer>\n","\n","  <script src=\"./script.js\"></script>\n","</body>\n","</html>"],"metadata":{"id":"hHNkRmVSQX2P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636568600,"user_tz":-360,"elapsed":18,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"bb216a59-5917-4ef0-b17e-a437773b0192"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/vehicle-counter/frontend/index.html\n"]}]},{"cell_type":"markdown","source":["### Frontend: CSS Styling\n","\n","This cell creates the `styles.css` file. It contains all the styling rules that control the visual appearance of our web application, such as colors, fonts, and layout, giving it a clean and modern look."],"metadata":{"id":"2imEgmQ9qIou"}},{"cell_type":"code","source":["%%writefile /content/vehicle-counter/frontend/styles.css\n",":root {\n","  --bg: #0b111b;\n","  --card: #121a26;\n","  --ink: #e8eef8;\n","  --muted: #a7b1c2;\n","  --up: #4ade80;\n","  --down: #60a5fa;\n","  --total: #a78bfa;\n","  --pill: #1f2a3b;\n","  --border: #223247;\n","  --error: #ef4444;\n","  --ok: #10b981;\n","}\n","* { box-sizing: border-box; }\n","html, body { height: 100%; }\n","body {\n","  margin: 0; background: var(--bg); color: var(--ink);\n","  font-family: Inter, system-ui, -apple-system, Segoe UI, Roboto, Arial, sans-serif;\n","}\n",".app-header { padding: 24px; text-align: center; }\n",".app-header h1 { margin: 0 0 6px; font-size: 28px; }\n",".subtitle { margin: 0; color: var(--muted); }\n",".container {\n","  max-width: 1100px; margin: 0 auto; padding: 12px 20px 40px;\n","  display: grid; grid-template-columns: 2fr 1fr; gap: 16px;\n","}\n","@media (max-width: 980px) { .container { grid-template-columns: 1fr; } }\n",".video-card, .counts-card {\n","  background: var(--card); border: 1px solid var(--border);\n","  border-radius: 12px; padding: 16px; box-shadow: 0 10px 30px rgba(0,0,0,0.25);\n","}\n",".video-header { display: flex; align-items: center; justify-content: space-between; margin-bottom: 10px; }\n",".video-header h2, .counts-card h2 { margin: 0; font-size: 18px; }\n",".status-bar {\n","  display: inline-flex; align-items: center; gap: 8px; background: var(--pill);\n","  border: 1px solid var(--border); border-radius: 999px; padding: 6px 10px; font-size: 12px; color: var(--muted);\n","}\n",".status-bar.ok { color: var(--ink); }\n",".status-bar.error { color: #ffd5d5; }\n",".dot { width: 10px; height: 10px; border-radius: 50%; background: var(--muted); }\n",".status-bar.ok .dot { background: var(--ok); animation: pulse 2s infinite; }\n",".status-bar.error .dot { background: var(--error); }\n","@keyframes pulse { 0% { box-shadow: 0 0 0 0 rgba(16,185,129,0.6);} 70% { box-shadow: 0 0 0 10px rgba(16,185,129,0);} 100% { box-shadow: 0 0 0 0 rgba(16,185,129,0);} }\n",".video-wrap { position: relative; width: 100%; background: #0c141f; border-radius: 8px; overflow: hidden; border: 1px solid var(--border); min-height: 240px; }\n","#video { display: block; width: 100%; height: auto; }\n",".overlay { position: absolute; inset: 0; background: rgba(0,0,0,0.75); display: flex; align-items: center; justify-content: center; color: #fff; font-size: 14px; gap: 10px; z-index: 2; }\n",".spinner { width: 22px; height: 22px; border-radius: 50%; border: 3px solid rgba(255,255,255,0.3); border-top-color: #fff; animation: spin .9s linear infinite; }\n","@keyframes spin { to { transform: rotate(360deg); } }\n",".controls { display: flex; gap: 10px; margin-top: 12px; flex-wrap: wrap; }\n",".btn { background: #1a2332; color: var(--ink); border: 1px solid var(--border); border-radius: 8px; padding: 10px 14px; font-weight: 600; font-size: 13px; cursor: pointer; }\n",".btn:hover { transform: translateY(-1px); }\n",".btn.primary { background: linear-gradient(135deg, #667eea, #764ba2); border: none; }\n",".counts-card .grid { display: grid; grid-template-columns: 1fr 1fr; gap: 12px; margin-top: 10px; }\n","@media (max-width: 520px) { .counts-card .grid { grid-template-columns: 1fr; } }\n",".metric { border: 1px solid var(--border); border-radius: 10px; padding: 16px; background: #0f1824; display: grid; grid-template-rows: auto auto auto; gap: 6px; }\n",".metric .label { color: var(--muted); font-size: 13px; }\n",".metric .value { font-size: 32px; font-weight: 700; letter-spacing: 1px; }\n",".metric .hint { color: var(--muted); font-size: 12px; }\n",".metric.up .value { color: var(--up); }\n",".metric.down .value { color: var(--down); }\n",".metric.total .value { color: var(--total); }\n",".meta { margin-top: 10px; color: var(--muted); font-size: 12px; text-align: right; }\n",".app-footer { text-align: center; padding: 22px 12px 32px; color: var(--muted); }"],"metadata":{"id":"2-4uTpoCQetp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636568632,"user_tz":-360,"elapsed":31,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"fc1d010e-dbaa-4bb6-eb18-487fe2f57867"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/vehicle-counter/frontend/styles.css\n"]}]},{"cell_type":"markdown","source":["### Frontend: JavaScript Logic\n","\n","This cell writes the `script.js` file, which brings the user interface to life. This code is responsible for:\n","* Connecting to the backend's video stream and API endpoints.\n","* Periodically fetching and displaying the latest vehicle counts and metrics.\n","* Handling clicks on the \"Reset,\" \"Pause,\" and \"Download CSV\" buttons."],"metadata":{"id":"rNHCBLimqLDO"}},{"cell_type":"code","source":["%%writefile /content/vehicle-counter/frontend/script.js\n","/**\n"," * Backend URL Configuration\n"," * --------------------------\n"," * This section determines the backend API URL to connect to.\n"," * It prioritizes in the following order:\n"," * 1. A 'backend' URL parameter (e.g., ?backend=http://192.168.1.10:5000). If found,\n"," * it's saved to localStorage and the page is reloaded without the parameter.\n"," * 2. A previously saved URL from localStorage.\n"," * 3. A default fallback URL (http://127.0.0.1:5000).\n"," */\n","const saved = localStorage.getItem(\"backend_url\");\n","const param = new URL(window.location.href).searchParams.get(\"backend\");\n","if (param) {\n","  // If a 'backend' URL parameter exists, save it for future visits.\n","  localStorage.setItem(\"backend_url\", param);\n","  const u = new URL(window.location.href);\n","  u.searchParams.delete(\"backend\"); // Clean the URL\n","  window.location.replace(u.toString()); // Reload the page with the clean URL\n","}\n","const BACKEND_BASE_URL = param || saved || \"http://127.0.0.1:5000\";\n","\n","// --- DOM Element References ---\n","// Get and cache references to all the HTML elements we'll need to interact with.\n","const videoEl = document.getElementById(\"video\");\n","const overlayEl = document.getElementById(\"loadingOverlay\");\n","const statusBar = document.getElementById(\"statusBar\");\n","const statusText = document.getElementById(\"statusText\");\n","const upEl = document.getElementById(\"count-up\");\n","const downEl = document.getElementById(\"count-down\");\n","const totalEl = document.getElementById(\"count-total\");\n","const rateUpEl = document.getElementById(\"rate-up\");\n","const rateDownEl = document.getElementById(\"rate-down\");\n","const fpsEl = document.getElementById(\"fps\");\n","const framesEl = document.getElementById(\"frames\");\n","const lastUpdatedEl = document.getElementById(\"last-updated\");\n","const backendUrlEl = document.getElementById(\"backendUrl\");\n","const btnReset = document.getElementById(\"btnReset\");\n","const btnPause = document.getElementById(\"btnPause\");\n","const btnDownload = document.getElementById(\"btnDownload\");\n","\n","// --- Constants and State Variables ---\n","// Configuration for update intervals.\n","const UPDATE_INTERVAL_MS = 1000; // How often to fetch new counts (1 second).\n","const HEALTH_INTERVAL_MS = 30000; // How often to check if the backend is alive (30 seconds).\n","const METRICS_INTERVAL_MS = 2000; // How often to fetch performance metrics like FPS (2 seconds).\n","const HISTORY_LIMIT = 900; // Maximum number of data points to store for the CSV download.\n","\n","// Application state variables.\n","let isPaused = false; // Toggles whether the app is polling for new data.\n","let countsInFlight = false; // Flag to prevent multiple simultaneous requests for counts.\n","let metricsInFlight = false; // Flag for metrics requests.\n","let healthInFlight = false; // Flag for health check requests.\n","let previousCounts = { up: 0, down: 0 }; // Store the last known counts to detect changes.\n","let startTime = Date.now(); // Timestamp for when the app started, used for calculating rates.\n","let countHistory = []; // Array to store historical data for CSV export.\n","\n","// --- Initial Setup ---\n","// Display the determined backend URL in the footer.\n","backendUrlEl.textContent = BACKEND_BASE_URL;\n","// Set the source of the video element to the backend's video feed endpoint.\n","videoEl.src = `${BACKEND_BASE_URL}/video_feed`;\n","\n","/**\n"," * Updates the UI status indicator.\n"," * @param {boolean} ok - If true, the status is 'ok' (green). If false, it's 'error' (red).\n"," * @param {string} msg - The message to display (e.g., \"Live\", \"Stream error\").\n"," */\n","function setStatus(ok, msg) {\n","  statusText.textContent = msg;\n","  statusBar.classList.toggle(\"ok\", !!ok);\n","  statusBar.classList.toggle(\"error\", !ok);\n","}\n","\n","// --- Video Element Event Handlers ---\n","// When the video stream loads successfully, hide the loading overlay and set status to \"Live\".\n","videoEl.onload = () => { overlayEl.style.display=\"none\"; setStatus(true,\"Live\"); };\n","// If there's an error loading the video, show the overlay and an error message.\n","videoEl.onerror = () => { overlayEl.style.display=\"flex\"; setStatus(false,\"Stream error — check backend\"); };\n","\n","/**\n"," * Fetches and updates the vehicle counts from the backend API.\n"," */\n","async function refreshCounts() {\n","  // Don't fetch if paused or if another request is already in progress.\n","  if (isPaused || countsInFlight) return;\n","  countsInFlight = true;\n","\n","  try {\n","    const res = await fetch(`${BACKEND_BASE_URL}/api/counts`, { cache: \"no-store\" });\n","    if (!res.ok) throw new Error(`HTTP ${res.status}`);\n","    const data = await res.json();\n","\n","    // Update UI with the new counts.\n","    const up = Number(data.up ?? 0);\n","    const down = Number(data.down ?? 0);\n","    const total = up + down;\n","    animateValue(upEl, up, previousCounts.up);\n","    animateValue(downEl, down, previousCounts.down);\n","    totalEl.textContent = total.toString();\n","\n","    // Calculate and display the rate (vehicles per minute).\n","    const elapsedMin = (Date.now() - startTime) / 60000;\n","    if (elapsedMin > 0) {\n","      rateUpEl.textContent = `${(up / elapsedMin).toFixed(1)} / min`;\n","      rateDownEl.textContent = `${(down / elapsedMin).toFixed(1)} / min`;\n","    }\n","\n","    // Store history for CSV download and manage its size.\n","    countHistory.push({ ts: new Date().toISOString(), up, down, total });\n","    if (countHistory.length > HISTORY_LIMIT) {\n","      countHistory.splice(0, countHistory.length - HISTORY_LIMIT);\n","    }\n","\n","    // Update state for the next cycle.\n","    previousCounts = { up, down };\n","    lastUpdatedEl.textContent = `Last updated: ${new Date().toLocaleTimeString()}`;\n","    if (statusBar.classList.contains(\"error\")) setStatus(true, \"Live\"); // Restore status if it was in error.\n","  } catch (err) {\n","    console.error(\"Counts fetch failed:\", err);\n","    setStatus(false, \"Backend unreachable\");\n","  } finally {\n","    countsInFlight = false; // Always reset the flag.\n","  }\n","}\n","\n","/**\n"," * Fetches and updates performance metrics like FPS from the backend.\n"," */\n","async function refreshMetrics() {\n","  if (metricsInFlight) return;\n","  metricsInFlight = true;\n","  try {\n","    const res = await fetch(`${BACKEND_BASE_URL}/api/metrics`, { cache: \"no-store\" });\n","    if (!res.ok) return;\n","    const m = await res.json();\n","    fpsEl.textContent = (m.fps ?? 0).toString();\n","    framesEl.textContent = `${m.frames_processed ?? 0} frames`;\n","  } catch {} // Fail silently if metrics aren't available.\n","  finally {\n","    metricsInFlight = false;\n","  }\n","}\n","\n","/**\n"," * Performs a health check to see if the backend is responsive.\n"," */\n","async function healthCheck() {\n","  if (healthInFlight) return;\n","  healthInFlight = true;\n","  try {\n","    const res = await fetch(`${BACKEND_BASE_URL}/health`, { cache: \"no-store\" });\n","    if (!res.ok) throw new Error(`HTTP ${res.status}`);\n","    const data = await res.json();\n","    if (data && data.ok) {\n","      if (statusBar.classList.contains(\"error\")) setStatus(true, \"Live\");\n","    } else {\n","      setStatus(false, \"Health check failed\");\n","    }\n","  } catch (e) {\n","    setStatus(false, \"Backend not responding\");\n","  } finally {\n","    healthInFlight = false;\n","  }\n","}\n","\n","// --- Polling Intervals ---\n","// Set up timers to repeatedly call the fetch functions.\n","setInterval(refreshCounts, UPDATE_INTERVAL_MS);\n","setInterval(refreshMetrics, METRICS_INTERVAL_MS);\n","setInterval(healthCheck, HEALTH_INTERVAL_MS);\n","\n","// Immediately call the functions once on page load to populate data.\n","refreshCounts();\n","refreshMetrics();\n","healthCheck();\n","\n","// --- Button Event Listeners ---\n","// Handle the \"Reset Counts\" button click.\n","btnReset.addEventListener(\"click\", async () => {\n","  try {\n","    const res = await fetch(`${BACKEND_BASE_URL}/api/reset`);\n","    if (!res.ok) throw new Error(`HTTP ${res.status}`);\n","    // Reset front-end state and UI to match the backend.\n","    previousCounts = { up: 0, down: 0 };\n","    startTime = Date.now();\n","    countHistory = [];\n","    upEl.textContent=\"0\"; downEl.textContent=\"0\"; totalEl.textContent=\"0\";\n","    rateUpEl.textContent=\"0.0 / min\"; rateDownEl.textContent=\"0.0 / min\";\n","    setStatus(true, \"Counts reset\");\n","  } catch (e) {\n","    setStatus(false, \"Reset failed\");\n","  }\n","});\n","\n","// Handle the \"Pause\" / \"Resume\" button click.\n","btnPause.addEventListener(\"click\", () => {\n","  isPaused = !isPaused;\n","  btnPause.textContent = isPaused ? \"Resume\" : \"Pause\";\n","  setStatus(true, isPaused ? \"Paused (polling stopped)\" : \"Live\");\n","  // If resuming, immediately fetch the latest data.\n","  if (!isPaused) {\n","    refreshCounts();\n","    refreshMetrics();\n","  }\n","});\n","\n","// Handle the \"Download CSV\" button click.\n","btnDownload.addEventListener(\"click\", () => {\n","  if (!countHistory.length) {\n","    // Using a custom modal or message would be better than alert in a real app.\n","    // For this PoC, alert is simple and effective.\n","    alert(\"No data to download yet.\");\n","    return;\n","  }\n","  // Convert the history array to a CSV string.\n","  let csv = \"Timestamp,Up,Down,Total\\n\";\n","  countHistory.forEach(r => { csv += `${r.ts},${r.up},${r.down},${r.total}\\n`; });\n","\n","  // Create a blob and trigger a download.\n","  const blob = new Blob([csv], { type: \"text/csv\" });\n","  const url = URL.createObjectURL(blob);\n","  const a = document.createElement(\"a\");\n","  a.href = url;\n","  a.download = `vehicle_counts_${new Date().toISOString()}.csv`;\n","  a.click(); // Programmatically click the link to start the download.\n","  URL.revokeObjectURL(url); // Clean up the created URL.\n","});\n","\n","/**\n"," * Animates a number change in an element by briefly scaling it up.\n"," * @param {HTMLElement} el - The element to animate.\n"," * @param {number} next - The new value.\n"," * @param {number} prev - The previous value.\n"," */\n","function animateValue(el, next, prev) {\n","  // Only animate if the value has actually changed.\n","  if (next !== prev) {\n","    el.textContent = next.toString();\n","    el.classList.add(\"pulse\");\n","    setTimeout(() => el.classList.remove(\"pulse\"), 250); // Remove class after animation.\n","  }\n","}\n","\n","// --- Dynamic CSS Injection ---\n","// Inject the 'pulse' animation style directly into the document's head.\n","// This keeps the JavaScript self-contained without needing a separate CSS file for this small effect.\n","const style = document.createElement(\"style\");\n","style.textContent = `.pulse { transform: scale(1.04); transition: transform .2s; }`;\n","document.head.appendChild(style);"],"metadata":{"id":"hM3vYYoVQhhB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636568677,"user_tz":-360,"elapsed":43,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"daf80f4d-3509-49b2-bc3a-af3b38ae7ab9"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing /content/vehicle-counter/frontend/script.js\n"]}]},{"cell_type":"markdown","source":["## 3. Install Dependencies\n","\n","Before we can run the backend, we need to install the necessary Python libraries. This cell uses `pip` to install `ultralytics` (for YOLOv8), `Flask` (the web server), `OpenCV` (for video processing), and other required packages."],"metadata":{"id":"Oa5s15LqqN01"}},{"cell_type":"code","source":["%pip install -q --upgrade pip setuptools wheel packaging\n","%pip uninstall -y -q ultralytics opencv-python opencv-contrib-python opencv-python-headless || true\n","\n","# Core deps\n","%pip install -q \"numpy>=2.1\" \"scipy>=1.14.1\" \"jedi\"\n","%pip install -q \"opencv-python-headless>=4.12.0.88\"\n","\n","# Ultralytics + Flask + CORS + SORT deps + ngrok\n","%pip install -q ultralytics flask==3.0.3 flask-cors==4.0.1 filterpy pyngrok\n","\n","# (Optional) If Torch isn’t present or mismatched, uncomment ONE of the following:\n","# CPU-only:\n","# %pip install -q torch --index-url https://download.pytorch.org/whl/cpu\n","# CUDA 12.x (common on Colab):\n","# %pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jM96bfKTIRyJ","executionInfo":{"status":"ok","timestamp":1758636630252,"user_tz":-360,"elapsed":61572,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"398f6a7b-ebc4-4d7a-df80-5fbf6078930c"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[33mWARNING: Skipping ultralytics as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, which is not installed.\n","albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, which is not installed.\n","dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.3 which is incompatible.\n","cupy-cuda12x 13.3.0 requires numpy<2.3,>=1.22, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dopamine-rl 4.1.2 requires opencv-python>=3.4.8.29, which is not installed.\n","tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n","numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[33m  DEPRECATION: Building 'filterpy' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'filterpy'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n","\u001b[0m  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"markdown","source":["### Verify Installations\n","\n","This is a quick check to confirm that the key libraries were installed correctly and to print their versions. This can be useful for debugging potential version conflicts."],"metadata":{"id":"x1B1c7ZBqQ48"}},{"cell_type":"code","source":["import importlib, cv2, torch, numpy, scipy\n","print(\"NumPy:\", numpy.__version__)\n","print(\"SciPy:\", scipy.__version__)\n","print(\"OpenCV:\", cv2.__version__)\n","print(\"Torch:\", torch.__version__)\n","print(\"Ultralytics:\", importlib.import_module(\"ultralytics\").__version__)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nnizFYrDI7Dv","executionInfo":{"status":"ok","timestamp":1758636638486,"user_tz":-360,"elapsed":8197,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"686bd0ad-5f97-417c-8477-56d0a1fb31f7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["NumPy: 2.0.2\n","SciPy: 1.16.2\n","OpenCV: 4.12.0\n","Torch: 2.8.0+cu126\n","Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n","Ultralytics: 8.3.203\n"]}]},{"cell_type":"markdown","source":["## 4. Configure the Application\n","\n","This is the **main configuration cell**. Before running the application, you should review and adjust these settings. The most important ones are:\n","* `VIDEO_PATH`: **You must set this** to the correct path of your video file (e.g., in your mounted Google Drive).\n","* `LINE_Y`: The vertical position (in pixels) of the counting line.\n","* `TRACKER_MODE`: Choose between `SORT` and `BYTE` trackers.\n","* `RESIZE_WIDTH`: A smaller width (e.g., 640) can significantly speed up processing."],"metadata":{"id":"oWsjHnbeqU82"}},{"cell_type":"code","source":["# ============================\n","# Environment setup for app.py\n","# ============================\n","\n","import os\n","\n","# ---- Required / commonly changed settings ----\n","\n","# Path to the input video file that YOLO will read frame-by-frame.\n","# Tip: upload a file to Colab and set this to its path.\n","os.environ[\"VIDEO_PATH\"] = \"/content/drive/MyDrive/Vehicle-counter/video1.mp4\"\n","\n","# Vertical pixel position of the counting line.\n","# Objects crossing this horizontal line are counted as \"up\" or \"down\".\n","os.environ[\"LINE_Y\"] = \"360\"\n","\n","# Resize width for each frame before detection (smaller = faster, but less detail).\n","# Keep aspect ratio; height is computed automatically.\n","os.environ[\"RESIZE_WIDTH\"] = \"960\"\n","\n","# What to do when the video loops back to the start:\n","# - \"true\"  → reset counts when the video restarts\n","# - \"false\" → keep counts across loops (good for demos)\n","os.environ[\"RESET_ON_LOOP\"] = \"false\"\n","\n","# Which tracker to use:\n","# - \"SORT\" → uses our custom SORT implementation (fast, simple)\n","# - \"BYTE\" → uses Ultralytics' ByteTrack via model.track()\n","os.environ[\"TRACKER_MODE\"] = \"SORT\"\n","\n","# Host/port for the Flask server.\n","# In Colab, keep host \"0.0.0.0\". If you use ngrok, it will tunnel to this port.\n","os.environ[\"HOST\"] = \"0.0.0.0\"\n","os.environ[\"PORT\"] = \"5000\"\n","\n","\n","# ---- Optional overrides (these already match app.py defaults) ----\n","# Change them only if you want different behavior.\n","\n","# YOLO model weights file. If not found locally, Ultralytics will download it.\n","os.environ[\"MODEL_PATH\"] = \"yolov8s.pt\"\n","\n","# Device selection:\n","# - \"auto\" → use CUDA GPU if available, otherwise CPU\n","# - \"cuda\" → force GPU (fails if no GPU)\n","# - \"cpu\"  → force CPU\n","os.environ[\"MODEL_DEVICE\"] = \"auto\"\n","\n","# Detection confidence threshold (higher = fewer, more confident detections).\n","os.environ[\"CONF_THRESH\"] = \"0.35\"\n","\n","# IoU threshold for NMS/association (controls how much boxes can overlap).\n","os.environ[\"IOU_THRESH\"] = \"0.50\"\n","\n","# How many recent center points to keep for drawing each track's trail.\n","os.environ[\"TRAIL_LEN\"] = \"20\"\n","\n","# JPEG quality for the MJPEG stream (0–100). Higher = better quality, larger bandwidth.\n","os.environ[\"JPEG_QUALITY\"] = \"80\"\n","\n","\n","# ---- (Optional) quick sanity printout ----\n","print(\"Video:\", os.environ[\"VIDEO_PATH\"])\n","print(\"Mode:\", os.environ[\"TRACKER_MODE\"])\n","print(\"Device:\", os.environ[\"MODEL_DEVICE\"])\n","print(\"Serving at http://localhost:\" + os.environ[\"PORT\"])\n"],"metadata":{"id":"I3SSgs6dQxOk","executionInfo":{"status":"ok","timestamp":1758636638556,"user_tz":-360,"elapsed":40,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b6a62c70-1fe1-4468-ae63-836e82a2a3ce"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Video: /content/drive/MyDrive/Vehicle-counter/video1.mp4\n","Mode: SORT\n","Device: auto\n","Serving at http://localhost:5000\n"]}]},{"cell_type":"markdown","source":["## 5. Run the Application\n","\n","With everything set up, let's launch the servers.\n","\n","### Launch the Backend Server\n","\n","This command starts the Flask web server (`app.py`) in the background. The `nohup` command ensures it keeps running, and its output is redirected to a log file named `backend.log`. We then display the end of the log file to check for any immediate errors."],"metadata":{"id":"CZO6j5dNqbpk"}},{"cell_type":"code","source":["%%bash\n","# ============================\n","# Launch the Flask backend in the background and show recent logs\n","# ============================\n","\n","# 1) Move into the backend folder that contains app.py\n","cd /content/vehicle-counter/backend\n","\n","# 2) Start the server in the background:\n","#    - `nohup` lets it keep running even if the notebook cell finishes\n","#    - `python app.py` runs your Flask app\n","#    - `> backend.log 2>&1` sends both stdout and stderr to backend.log\n","#    - `&` runs it in the background so the cell doesn’t block\n","nohup python app.py > backend.log 2>&1 &\n","\n","# 3) Give the server a moment to start (download model, bind to port, etc.)\n","sleep 2\n","\n","# 4) Show the last 30 lines of the log so you can confirm it’s running\n","#    - `|| true` prevents the cell from failing if the file is empty or missing\n","tail -n 30 backend.log || true"],"metadata":{"id":"gFjANNkHQywc","executionInfo":{"status":"ok","timestamp":1758636640561,"user_tz":-360,"elapsed":2003,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Expose the Backend with a Public URL\n","\n","The Flask server is running inside the Colab environment, which isn't directly accessible from the internet. This cell downloads and runs **Cloudflare Tunnel** to create a secure, temporary public URL that forwards to our backend server on port 5000.\n","\n","**The URL generated by this cell is your backend endpoint.**"],"metadata":{"id":"k7GxCcC3qi0f"}},{"cell_type":"code","source":["%%bash\n","cd /content\n","# Download cloudflared (no account needed)\n","wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n","chmod +x cloudflared\n","# Open a tunnel to the backend on port 5000\n","nohup ./cloudflared tunnel --url http://localhost:5000 --no-autoupdate > cf_backend.log 2>&1 &\n","sleep 5 # Increased sleep time\n","echo \"Backend tunnel logs (showing URL):\"\n","grep -o \"https://.*trycloudflare.com\" -m 1 cf_backend.log || tail -n 50 cf_backend.log"],"metadata":{"id":"iWgHdhpcQ3sO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636646132,"user_tz":-360,"elapsed":5551,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"87a74d22-af19-4533-c735-8118ae704be8"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Backend tunnel logs (showing URL):\n","https://privilege-identifying-formatting-novel.trycloudflare.com\n"]}]},{"cell_type":"markdown","source":["### Launch the Frontend and Expose Its Public URL\n","\n","Next, we need to serve our HTML, CSS, and JS files.\n","1.  A simple Python HTTP server is started in the `frontend` directory.\n","2.  Cloudflare Tunnel is used again to create a second public URL for this frontend server.\n","\n","**This is the main URL you should open in your browser to view the application.**"],"metadata":{"id":"lTwOL7qJqnxt"}},{"cell_type":"code","source":["%%bash\n","cd /content/vehicle-counter/frontend\n","nohup python -m http.server 8000 > fe.log 2>&1 &\n","sleep 2\n","cd /content\n","nohup ./cloudflared tunnel --url http://localhost:8000 --no-autoupdate > cf_frontend.log 2>&1 &\n","sleep 5 # Increased sleep time\n","echo \"Frontend tunnel logs (showing URL):\"\n","grep -o \"https://.*trycloudflare.com\" -m 1 cf_frontend.log || tail -n 50 cf_frontend.log"],"metadata":{"id":"n-B0ifJ5Q48G","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758636653220,"user_tz":-360,"elapsed":7085,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"15dc9ec8-0ed4-4075-bee7-15ceebbdf1df"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Frontend tunnel logs (showing URL):\n","https://lan-relative-motor-arthritis.trycloudflare.com\n"]}]},{"cell_type":"markdown","source":["## 6. Access Your Application\n","\n","The final step is to launch the web interface. The code cell below automates this process by doing the following:\n","\n","1.  It reads the log files that were created when we launched the public tunnels.\n","2.  It automatically finds and extracts the unique public URLs for both the frontend and backend servers.\n","3.  It combines these two URLs into a single, final link, correctly passing the backend address to the frontend.\n","\n","A styled, clickable button will appear in the output below. **Click this link to open your application in a new browser tab.**"],"metadata":{"id":"1PEu6HWKqq_o"}},{"cell_type":"code","source":["import re\n","import os\n","from IPython.display import display, HTML\n","\n","# --- Configuration ---\n","BACKEND_LOG_FILE = '/content/cf_backend.log'\n","FRONTEND_LOG_FILE = '/content/cf_frontend.log'\n","\n","# --- Function to extract URL from a log file ---\n","def find_url_in_log(log_file):\n","    \"\"\"Searches a log file for the first Cloudflare URL and returns it.\"\"\"\n","    if not os.path.exists(log_file):\n","        return None\n","    try:\n","        with open(log_file, 'r') as f:\n","            for line in f:\n","                # Use regex to find the URL\n","                match = re.search(r'(https?://[a-zA-Z0-9-]+\\.trycloudflare\\.com)', line)\n","                if match:\n","                    return match.group(1)\n","    except Exception as e:\n","        print(f\"Error reading {log_file}: {e}\")\n","    return None\n","\n","# --- Main script ---\n","backend_url = find_url_in_log(BACKEND_LOG_FILE)\n","frontend_url = find_url_in_log(FRONTEND_LOG_FILE)\n","\n","print(\"--- URL Extraction ---\")\n","print(f\"Backend Log: {BACKEND_LOG_FILE}\")\n","print(f\"Frontend Log: {FRONTEND_LOG_FILE}\")\n","print(\"-\" * 20)\n","\n","if backend_url:\n","    print(f\"✅ Backend URL Found: {backend_url}\")\n","else:\n","    print(f\"❌ Backend URL not found. Check '{BACKEND_LOG_FILE}' for errors.\")\n","\n","if frontend_url:\n","    print(f\"✅ Frontend URL Found: {frontend_url}\")\n","else:\n","    print(f\"❌ Frontend URL not found. Check '{FRONTEND_LOG_FILE}' for errors.\")\n","\n","print(\"-\" * 20)\n","\n","# --- Display the final clickable link ---\n","if backend_url and frontend_url:\n","    final_url = f\"{frontend_url}/?backend={backend_url}\"\n","\n","    # Create an HTML link with improved styling for better visibility\n","    link_html = (\n","        f'<div style=\"font-family: -apple-system, BlinkMacSystemFont, \\'Segoe UI\\', Roboto, Helvetica, Arial, sans-serif; max-width: 600px; margin: 25px auto; padding: 25px; border-radius: 12px; border: 1px solid #e0e0e0; background: linear-gradient(135deg, #f9f9f9, #ffffff); box-shadow: 0 10px 25px rgba(0,0,0,0.08); text-align: center;\">'\n","        f'<h2 style=\"color: #2c3e50; font-size: 24px; margin-bottom: 10px;\"> Your Application is Ready!</h2>'\n","        f'<p style=\"color: #555; font-size: 16px; margin-bottom: 25px;\">Click the button below to open the Vehicle Counter:</p>'\n","        f'<a href=\"{final_url}\" target=\"_blank\" style=\"display: inline-block; font-size: 18px; font-weight: bold; color: #ffffff; background: linear-gradient(135deg, #3498db, #2980b9); padding: 14px 28px; border-radius: 8px; text-decoration: none; box-shadow: 0 5px 15px rgba(52, 152, 219, 0.4); transition: all 0.3s ease;\">'\n","        f'Open Application'\n","        f'</a>'\n","        f'<p style=\"font-size: 12px; margin-top: 20px; color: #7f8c8d;\">'\n","        f'Or copy this URL:'\n","        f'<br><code style=\"background-color: #ecf0f1; padding: 4px 8px; border-radius: 4px; color: #2c3e50; word-break: break-all; display: inline-block; margin-top: 5px;\">{final_url}</code>'\n","        f'</p>'\n","        f'</div>'\n","    )\n","\n","    display(HTML(link_html))\n","else:\n","    error_html = (\n","        '<div style=\"font-family: -apple-system, BlinkMacSystemFont, \\'Segoe UI\\', Roboto, Helvetica, Arial, sans-serif; max-width: 600px; margin: 25px auto; padding: 25px; border-radius: 12px; border: 1px solid #f5c6cb; background: linear-gradient(135deg, #fff5f5, #ffebeb); box-shadow: 0 10px 25px rgba(0,0,0,0.08); text-align: center;\">'\n","        '<h2 style=\"color: #721c24; font-size: 24px;\">⚠️ Could Not Generate Link</h2>'\n","        '<p style=\"color: #721c24; font-size: 16px;\">One or both of the necessary URLs could not be found. Please scroll up and check the output of the cells that run the Cloudflare Tunnels for any errors.</p>'\n","        '</div>'\n","    )\n","    display(HTML(error_html))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"id":"ftoQC_DsnTBQ","executionInfo":{"status":"ok","timestamp":1758636653280,"user_tz":-360,"elapsed":57,"user":{"displayName":"Imtiaz Ahmed","userId":"09533839672172316907"}},"outputId":"72410c76-1a2d-4b2e-c4ee-0f840ca4cf04"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["--- URL Extraction ---\n","Backend Log: /content/cf_backend.log\n","Frontend Log: /content/cf_frontend.log\n","--------------------\n","✅ Backend URL Found: https://privilege-identifying-formatting-novel.trycloudflare.com\n","✅ Frontend URL Found: https://lan-relative-motor-arthritis.trycloudflare.com\n","--------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; max-width: 600px; margin: 25px auto; padding: 25px; border-radius: 12px; border: 1px solid #e0e0e0; background: linear-gradient(135deg, #f9f9f9, #ffffff); box-shadow: 0 10px 25px rgba(0,0,0,0.08); text-align: center;\"><h2 style=\"color: #2c3e50; font-size: 24px; margin-bottom: 10px;\"> Your Application is Ready!</h2><p style=\"color: #555; font-size: 16px; margin-bottom: 25px;\">Click the button below to open the Vehicle Counter:</p><a href=\"https://lan-relative-motor-arthritis.trycloudflare.com/?backend=https://privilege-identifying-formatting-novel.trycloudflare.com\" target=\"_blank\" style=\"display: inline-block; font-size: 18px; font-weight: bold; color: #ffffff; background: linear-gradient(135deg, #3498db, #2980b9); padding: 14px 28px; border-radius: 8px; text-decoration: none; box-shadow: 0 5px 15px rgba(52, 152, 219, 0.4); transition: all 0.3s ease;\">Open Application</a><p style=\"font-size: 12px; margin-top: 20px; color: #7f8c8d;\">Or copy this URL:<br><code style=\"background-color: #ecf0f1; padding: 4px 8px; border-radius: 4px; color: #2c3e50; word-break: break-all; display: inline-block; margin-top: 5px;\">https://lan-relative-motor-arthritis.trycloudflare.com/?backend=https://privilege-identifying-formatting-novel.trycloudflare.com</code></p></div>"]},"metadata":{}}]}]}